@phdthesis{jamil2018phdthesis,
  title={Regression modelling using priors depending on Fisher information covariance kernels (I-priors)},
  author={\HJ},
  school={London School of Economics and Political Science},
  year=2018,
  month=10
}

@phdthesis{beal2003variational,
	Author = {Beal, Matthew James},
	Date-Modified = {2018-06-06 3:27:31 pm +0000},
	School = {Gatsby Computational Neuroscience Unit, University College London},
	Title = {Variational algorithms for approximate Bayesian inference},
	Year = {2003}}

@article{Kass1995,
author = {Kass, Robert and Raftery, Adrian},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kass, Raftery/Journal of the American Statistical Association/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
pages = {773--795},
title = {{Bayes Factors}},
volume = {90},
year = {1995}
}

@book{Murphy1991,
abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
author = {Murphy, Kevin P.},
booktitle = {Machine Learning: A Probabilistic Perspective},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Murphy/Machine Learning A Probabilistic Perspective/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:pdf},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012}
}

@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bishop/Unknown/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}

@article{erosheva2007describing,
  title={Describing disability through individual-level mixture models for multivariate binary data},
  author={Erosheva, Elena A and Fienberg, Stephen E and Joutard, Cyrille},
  journal={\annas},
  volume={1},
  number={2},
  pages={346},
  year={2007},
  publisher={NIH Public Access}
}

@article{grimmer2010introduction,
  title={An introduction to Bayesian inference via variational approximations},
  author={Grimmer, Justin},
  journal={Political Analysis},
  volume={19},
  number={1},
  pages={32--47},
  year={2010},
  publisher={Oxford University Press}
}

@article{westling2017,
archivePrefix = {arXiv},
arxivId = {1510.08151},
author = {Ted Westling and Tyler H. McCormick},
eprint = {1510.08151},
title = {{Beyond prediction: A framework for inference with variational approximations in mixture models}},
year = {2017}
}

@article{wang2017,
archivePrefix = {arXiv},
arxivId = {1512.08731},
author = {Y. Samuel Wang and Ross Matsueda and Elena A. Erosheva},
eprint = {1512.08731},
title = {{A variational EM method for mixed membership models with multivariate rank data: An analysis of public policy preferences}},
year = {2017}
}

@inproceedings{beal2003,
  author       = {M. J. Beal and Z. Ghahramani},
  editor       = {José M. Bernardo and A. Philip Dawid and James O. Berger and Mike West and David Heckerman and M.J. Bayarri and Adrian F.M. Smith},
  title        = {{The variational Bayesian EM algorithm for incomplete data: With application to scoring graphical model structures}},
  date         = {2003},
  booktitle    = {Bayesian Statistics 7},
%  booksubtitle = {Relativistic groups and analyticity},
  booktitleaddon= {Proceedings of the Seventh Valencia International Meeting},
%  eventdate    = {1968-05-19/1968-05-25},
%  venue        = {Aspen{\"a}sgarden, Lerum},
  publisher    = {Oxford University Press},
  location     = {Oxford},
  pages        = {453--464}
}

@article{blei2017variational,
	Author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	Date-Modified = {2018-06-06 4:35:16 pm +0000},
	%Doi = {10.1080/01621459.2017.1285773},
    journal={\jasa},	
    Number = {518},
	Pages = {859--877},
	Publisher = {Taylor \& Francis},
	Title = {Variational Inference: A Review for Statisticians},
	Volume = {112},
	Year = {2017}
}

@unpublished{bleiyoutube,
author = {Blei, David M},
%note = {Computational Challenges in Machine Learning.},
title = {{Variational Inference: Foundations and Innovations}},
year = {2017},
URL = {https://simons.berkeley.edu/talks/david-blei-2017-5-1}
}

@article{gunawardana2005convergence,
  title={Convergence theorems for generalized alternating minimization procedures},
  author={Gunawardana, Asela and Byrne, William},
  journal={{Journal of Machine Learning Research}},
  volume={6},
  pages={2049--2073},
  year={2005}
}

@article{zhao2013,
  title={{Diagnostics for variational Bayes approximations}},
  author={Hui Zhao and Paul Marriott},
  arxivId = {1309.5117},
  eprint = {1309.5117},
  year = {2013},
  archivePrefix = {arXiv}
}



